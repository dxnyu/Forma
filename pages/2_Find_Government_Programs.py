# From public libraries
import streamlit as st
import os
import shutil

from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
# from langchain.chains import RetrievalQA
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from logics import user_query_handler2 

st.title('Find out about government programs offered by the Singapore Government here.')

form = st.form(key="form")
form.subheader("Prompt")

user_prompt = form.text_area("Enter your prompt here", height=150)

urls = ["https://taxsummaries.pwc.com/singapore/corporate/tax-credits-and-incentives", "https://www.aseanbriefing.com/doing-business-guide/singapore/why-singapore/incentives-for-doing-business-in-singapore"]

llm = ChatOpenAI(model='gpt-4o-mini')
embeddings = OpenAIEmbeddings(model='text-embedding-3-small')

compiled_text = user_query_handler2.content_compiler(urls)
split_text = user_query_handler2.splitter(compiled_text)

vector_store = Chroma.from_documents(
    collection_name = "Gov_Support",
    documents = split_text,
    embedding = embeddings,
    persist_directory = "./chroma_db"
)

if form.form_submit_button("Submit"):
    # Get user input
    st.toast(f"User Input Submitted - {user_prompt}")

    prompt = ChatPromptTemplate.from_template("""
    Use the following context to answer the user's question.

    Context:
    {context}

    Question:
    {input}
    """)

    chain = create_retrieval_chain(
        retriever=vector_store.as_retriever(k=5),
        combine_docs_chain=create_stuff_documents_chain(llm, prompt)
    )

    response = chain.invoke({"input": user_prompt})

    # Display response generated by the LLM onto the frontend 
    st.write(response["answer"])
    print(response)

    vector_store.delete_collection()



