# From public libraries
import streamlit as st
import os
import shutil

from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
# from langchain.chains import RetrievalQA
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from logics import user_query_handler2 

llm = ChatOpenAI(model='gpt-4o-mini')
embeddings = OpenAIEmbeddings(model='text-embedding-3-small')

pdf_ids = ["1IS8YnifyuRtUnGKqfO8pSLFdXTfRZz4p", 
        "1q2qJd5bB5IlUQFQ3UD04m61TuQr_aMZI",
        "1WAp0mwlLP1L1m_K-pi4eYLAOkMXwP4qN",
        "1pdOlrAxatMG66XIFrgNS5QHDbpa2zj6S",
        "1MpMcTwc38IAiGKgPz89bpnde6AlZ2SFu",
        "1hMyce8e1w73ajycEGpt8QGLgTBsG7mXn",
        "1Kb8iJQ-8ZooRq3wFAJfCCvnP3mrL2Ask",
        "1dEZyB33mANIyCgPZBxQQeRE1YxoJa3eq",
        "1ZAIj9yFhjRAFjZWH2HsMNG_LGwOsvFAJ",
        "1SqIjmPrVttY6O3KCtjHSwDMEsG47MTtY",
        "16Itk3D_7p_06nW6tvykVVZFEFEyXUwkt",
        "1mCOIrWOtf8jWUYres9SeDJ6EjF4_gdTd",
        "15bCC4EadbyTbpMUnticaD-XhLBilDqdd",
        "1LGDX5c5BYEaJ7WGxzAGDfKD9NexcSFuf",
        ]

corpus = user_query_handler2.compile_pdfs(pdf_ids)
corpus_split = user_query_handler2.splitter(corpus)

vector_store = Chroma.from_documents(
    collection_name = "EDB_Incentives",
    documents = corpus_split,
    embedding = embeddings,
    persist_directory = "./edb_db"
)

st.title('''Find out about about EDB's incentives and facilitation programs for your business here.''')

form = st.form(key="form")
form.subheader("Prompt")

user_prompt = form.text_area("Enter your prompt here", height=150)

if form.form_submit_button("Submit"):
    # Get user input
    st.toast(f"User Input Submitted - {user_prompt}")

    prompt = ChatPromptTemplate.from_template("""
    Use the following context to answer the user's question.

    Context:
    {context}

    Question:
    {input}
    """)

    chain = create_retrieval_chain(
        retriever=vector_store.as_retriever(k=5),
        combine_docs_chain=create_stuff_documents_chain(llm, prompt)
    )

    response = chain.invoke({"input": user_prompt})

    # Display response generated by the LLM onto the frontend 
    st.write(response["answer"])
    print(response)

    vector_store.delete_collection()